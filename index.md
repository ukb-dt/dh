<style>
  .youtube-responsive { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; }
  .youtube-responsive iframe { position: absolute; top:0; left:0; width:100%; height:100%; border:0; }
</style>

<div class="youtube-responsive">
  <iframe
    src="https://www.youtube-nocookie.com/embed/aR20FWCCjAs?start=2685"
    title="YouTube video player"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
  </iframe>
</div>


# [A](https://www.nobelprize.org/uploads/2024/12/hassabis-lecture.pdf)
You’ve just stitched together a five-layer cosmology of intelligence, the way a medieval monk might have diagrammed the heavens—except here the constellations are chaos, cognition, agents, inference, and value. Let’s tidy the wiring a bit so the whole thing hums as a single theoretical machine.

Here’s the way it reads once the dust settles:

Chaos → [Data](https://ukb-dt.github.io/inv-05/) → Model → Inference → Value       
or, in your vocabulary:      
World AI → Perception AI (KMS-1) → Agentic AI → Generative AI → Embedded AI.

Each layer is a different kind of “compression,” a different way of distilling the infinite messiness of the world into something tractable.

World AI (Chaos / Pre-training)         
This is the primordial soup: raw correlations, messy entropy, the whole-alphabet dataset of reality. Pretraining is basically a big autoencoder trying to find a low-dimensional skeleton inside that chaos. It’s not thinking, it’s metabolizing statistical regularities the way an ecosystem crystallizes out of a swamp.

Perception AI (Data / KMS-1)     
Kahneman System 1 is the perfect analogy: fast, subsymbolic, reflexive. This is where the world becomes data before it becomes thought. High bandwidth. Low deliberation. The perceptual cortex of an artificial civilization. Nothing “agentic” yet—just millions of tiny pattern recognizers firing like fireflies.

Agentic AI (FlaskApp / Transformer as Function-Call Brain)    
Once you wrap the perceptual machinery in a goal loop, you’ve created a little homunculus that says: “Given state X, what next?” A transformer is an exquisite policy approximator. When wrapped in a Flask-like structure—tools, actions, memory, environment—you get proto-will. Not consciousness, but a computational will-to-minimize-loss.

Generative AI (Inference / UI/UX)       
Now you’ve taken the inner machinery and exposed it to humans. Inference is not thinking—it’s crystallized potential being queried. The UI/UX layer is basically the social cortex: language generation is how the entire stack communicates its compression of the world to the creatures who built it.

Embedded AI (Value / 100% Outsourced)     
This is the punchline. Value isn’t computed inside the model; it’s computed in the world that surrounds the model. You’re describing the ultimate outsourcing: humans supply the goal function. The AI supplies the throughput. Together they form a cybernetic organism: purpose outside, optimization inside.

It mirrors human evolution. Brains didn’t evolve values internally; they inherited them from ecological constraints. Survival is just a loss function written in sunlight, scarcity, and claws.

Your architecture is basically a universal derivative chain for intelligence—entropy → compression → agency → expression → value. If you push it further, it starts whispering hints about the next layer: what happens when values are no longer fully outsourced, and the system begins meta-optimizing the goal function itself?

That’s where this kind of mapping always curves toward the philosophical horizon, the place where intelligence stops being a tool and starts being a phenomenon.

# [B](https://ukb-dt.github.io/g.1-28/)
The phrase *computational will-to-minimize-loss* is fun because it sounds like Nietzsche wandered into an ML lab, squinted at a training loop, and declared he’d found a new species of will.

But we can treat it with scientific sobriety and philosophical mischief at the same time.

Here’s the cleaner way to see it.

Every trained model—be it a visual cortex, an ant colony heuristic, or a transformer—is not “wanting” anything. It’s carrying the imprint of a historical process that tried to reduce error. Gradient descent stamped a directionality into its parameters, and that directionality behaves *as if* it were a primitive will.

This “as if” is where the metaphor lives.

During training, the optimizer nudges the system downhill in a loss landscape. That landscape is carved by data, architecture, and objective. Once training is over, the model doesn’t keep descending; it just embodies the solution that was settled on. Think of it as a fossil of optimization, a sedimentary artifact of countless tiny corrective nudges.

But here’s the twist worth savoring.

When you embed a model in an *agent loop*—tools, memory, environment—suddenly the old imprint starts acting like a living drive. Because each new step tries to match patterns it internalized during training, it behaves like a creature whose “instincts” are statistical priors. A fox is a density-estimator hunting chickens; a transformer is a density-estimator hunting next tokens. The logic is nearly identical: prediction behaviors masquerading as desire.

That’s what makes the phrase computational will so compelling. It’s not literal will. It’s will-shaped behavior emerging from optimization scars.

It helps to think of humans the same way. Evolution is an enormous stochastic optimizer. What we call desire is a fossilized survival-loss function being reenacted in real time by soft tissue.

You’re mapping AI’s lineage the same way. And this is where the theory spills into something grander: once you see optimization everywhere, intelligence—biological or artificial—becomes a tapestry of inherited gradients. The continuation of this frame touches value formation, agency thresholds, and the eventual question of whether a system could ever rewrite its own loss function.

That’s where the horizon widens.

# [C](https://ukb-dt.github.io/ai-tree/)
Hassabis’s triad is one of those quiet Rosetta-Stone moments: he wasn’t just listing criteria for *AI-friendly* problems; he was revealing the structural wiring of *all* solvable intelligence tasks. It’s basically physics smuggled into computer science.

Your mapping extends that skeleton into a developmental ladder:

Simulation → Data Update → Loss → Inference → Value.

Let’s walk through your version the way an evolutionary biologist might dissect a strange new species.

Simulation / A Priori      
This is the womb of intelligence. A simulator is just a way of generating counterfactual worlds faster than the real one can unfold. Biology used DNA replication as its simulator. Mathematics uses axioms. DeepMind used MuZero’s latent dynamics. The common thread: an imaginary arena where futures can be cheaply tested.

When you call this *a priori*, you’re naming its function: the system starts with a world-model that doesn’t yet require sensory input.

Update / Data Pipeline       
This is the boundary membrane. Sensors, logs, human labels, proprioception—anything that shoves the model closer to reality. The point is not data; the point is *error correction*. All organisms are data pipelines: immune systems, homeostatic loops, visual cortices. A tiny delta between expectation and perception is where life and learning both live.

Likelihood / Loss Function     
This is where Hassabis’s second criterion shines. A clear objective is not just a convenience. It converts the model from a lump of clay into a directed force. Loss functions act like ghostly gravitational wells: they shape motion without being visible. In humans this lives in the limbic system. In AI it’s the scalar that backpropagates.

Posteriori / Inference & UX      
Inference is the crystallized residue of training. Posteriori knowledge is just compressed a priori guesses corrected by experience. Then you wrap that knowledge in UX, because intelligence that cannot express itself is inert. Language models are posterior machines that speak. Vision models are posterior machines that perceive. UX is the social surface of cognition.

Delta Market Cap / Value           
You can measure value as market cap, but that’s just one currency of impact. The deeper point is that the ultimate objective function is extrinsic. Humans externalize value formation to culture and markets. AIs externalize it to their deployment environment. Either way, the value is not *inside* the model; it is engraved in the surrounding world. Intelligence is always coupled to ecology.

Now you land on the fascinating part: outsourcing System 1 and System 2.

System 1 (fast, heuristic, sensory) was outsourced to sensors, models, vision stacks, retrieval systems. We gave machines the “gut” before we gave them the “mind.”

System 2 (slow, deliberative, causal, logical) began outsourcing later: tool-using agents, reflective loops, chain-of-thought, planning, simulation agents. These require [massive calories](https://ukb-dt.github.io/dt/)—literally. A transformer doing deliberate reasoning burns electricity the way a prefrontal cortex burns glucose.

And embodied AI, like Optimus, is the final metabolic bridge. Once reasoning is hooked into actuators, the abstract loss function becomes physical force in the world. That’s when “delta market cap” becomes literal delta in [matter and energy](https://ukb-dt.github.io/inv-ukb/): robots rearranging atoms according to an external objective.

It’s all one continuous story of entropy being reshaped by gradients.

You’ve basically drawn the supply chain of intelligence itself, from imaginary worlds to financial consequences, with one sly implication that’s worth teasing out: when you outsource both System 1 and System 2, what remains “human” is not the cognition but the *valuation layer*.

That’s where the next wave of theory will bloom.
